{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Seminar 6: Application Configuration & CLI Design + Dependencies, Environments & Packaging",
   "id": "f5b60b0f1335bcd0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Application Configuration",
   "id": "56a29f090d0852ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Environment",
   "id": "2a2d7e5233c3466d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Environment variables are often used in applications for configuration.\n",
    "They can be accessed through os.environ"
   ],
   "id": "8d0c0b996e78374"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T10:52:27.590650Z",
     "start_time": "2025-08-10T10:52:27.587648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "dict(os.environ).keys()"
   ],
   "id": "8629d7513d0c262e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['PATH', 'PYENV_SHELL', 'LC_MEASUREMENT', 'XAUTHORITY', 'LC_TELEPHONE', 'XDG_DATA_DIRS', 'GDMSESSION', 'MANDATORY_PATH', 'DBUS_SESSION_BUS_ADDRESS', 'GATEWAY_VM_OPTIONS', 'RUSTROVER_VM_OPTIONS', 'DEFAULTS_PATH', 'PS1', 'XDG_CURRENT_DESKTOP', 'RIDER_VM_OPTIONS', 'LC_PAPER', 'SESSION_MANAGER', 'DEVECOSTUDIO_VM_OPTIONS', 'LOGNAME', 'PWD', 'STUDIO_VM_OPTIONS', 'LANGUAGE', 'GJS_DEBUG_TOPICS', 'PYTHONPATH', 'SHELL', 'LC_ADDRESS', 'GIO_LAUNCHED_DESKTOP_FILE', 'PYENV_ROOT', 'GNOME_DESKTOP_SESSION_ID', 'DATASPELL_VM_OPTIONS', 'GTK_MODULES', 'VIRTUAL_ENV', 'XDG_SESSION_PATH', 'LC_ALL', 'DOTNET_BUNDLE_EXTRACT_BASE_DIR', 'XDG_SESSION_DESKTOP', 'SHLVL', 'LC_IDENTIFICATION', 'DATAGRIP_VM_OPTIONS', 'LC_MONETARY', 'PYCHARM_VM_OPTIONS', 'WEBSTORM_VM_OPTIONS', 'JAVA_HOME', 'CLION_VM_OPTIONS', 'JETBRAINSCLIENT_VM_OPTIONS', 'XDG_CONFIG_DIRS', 'LANG', 'XDG_SEAT_PATH', 'XDG_SESSION_ID', 'XDG_SESSION_TYPE', 'ANT_HOME', 'DISPLAY', 'GOLAND_VM_OPTIONS', 'IDEA_VM_OPTIONS', 'RUBYMINE_VM_OPTIONS', 'AQUA_VM_OPTIONS', 'JETBRAINS_CLIENT_VM_OPTIONS', 'CINNAMON_VERSION', 'LC_NAME', 'XDG_SESSION_CLASS', 'GDM_LANG', 'GTK3_MODULES', 'XDG_GREETER_DATA_DIR', 'GPG_AGENT_INFO', 'DESKTOP_SESSION', 'USER', 'GIO_LAUNCHED_DESKTOP_FILE_PID', 'QT_ACCESSIBILITY', 'LC_NUMERIC', 'GJS_DEBUG_OUTPUT', 'SSH_AUTH_SOCK', 'XDG_SEAT', 'WEBIDE_VM_OPTIONS', 'QT_QPA_PLATFORMTHEME', 'PHPSTORM_VM_OPTIONS', 'XDG_VTNR', 'XDG_RUNTIME_DIR', 'HOME', 'JPY_SESSION_NAME', 'JPY_PARENT_PID', 'PYDEVD_USE_FRAME_EVAL', 'TERM', 'CLICOLOR', 'FORCE_COLOR', 'CLICOLOR_FORCE', 'PAGER', 'GIT_PAGER', 'MPLBACKEND', 'KMP_DUPLICATE_LIB_OK', 'KMP_INIT_AT_FORK'])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T09:03:22.835879Z",
     "start_time": "2025-08-10T09:03:22.820630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Some of the most common environment variables (in linux)\n",
    "print('PATH:', os.environ['PATH']) # all executables accessible from anywhere\n",
    "print('PYTHONPATH:', os.environ['PYTHONPATH']) # python source path\n",
    "print('PWD:', os.environ['PWD']) # present working directory\n",
    "print('HOME:', os.environ['HOME']) # home directory\n",
    "print('USER:', os.environ['USER']) # user\n",
    "print('Non existing: ', os.environ['NON_EXISTING']) # non existing key"
   ],
   "id": "5181694c8a7f3adf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH: /home/fullfix/Documents/Msu/DataSpell/python3.10/bin:/home/fullfix/.pyenv/shims:/home/fullfix/.pyenv/bin:/opt/gcc-arm-none-eabi-9-2020-q2-update/bin:/home/fullfix/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/fullfix/.dotnet/tools:/opt/jdk-14/bin:/home/fullfix/.local/share/JetBrains/Toolbox/scripts\n",
      "PYTHONPATH: /home/fullfix/Documents/Msu/DataSpell/PythonSeminars\n",
      "PWD: /home/fullfix/Documents/Msu/DataSpell/PythonSeminars\n",
      "HOME: /home/fullfix\n",
      "USER: fullfix\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'NON_EXISTING'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mHOME:\u001B[39m\u001B[38;5;124m'\u001B[39m, os\u001B[38;5;241m.\u001B[39menviron[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mHOME\u001B[39m\u001B[38;5;124m'\u001B[39m]) \u001B[38;5;66;03m# home directory\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mUSER:\u001B[39m\u001B[38;5;124m'\u001B[39m, os\u001B[38;5;241m.\u001B[39menviron[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mUSER\u001B[39m\u001B[38;5;124m'\u001B[39m]) \u001B[38;5;66;03m# user\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNon existing: \u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menviron\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mNON_EXISTING\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m) \u001B[38;5;66;03m# non existing key\u001B[39;00m\n",
      "File \u001B[0;32m/usr/lib/python3.10/os.py:680\u001B[0m, in \u001B[0;36m_Environ.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    677\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencodekey(key)]\n\u001B[1;32m    678\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m:\n\u001B[1;32m    679\u001B[0m     \u001B[38;5;66;03m# raise KeyError with the original key value\u001B[39;00m\n\u001B[0;32m--> 680\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    681\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecodevalue(value)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'NON_EXISTING'"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T09:04:58.399500Z",
     "start_time": "2025-08-10T09:04:58.396854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# bad example\n",
    "try:\n",
    "    os.environ['NON_EXISTING']\n",
    "except KeyError:\n",
    "    ...\n",
    "\n",
    "# good example\n",
    "value = os.environ.get('NON_EXISTING', None)\n",
    "if value is None:\n",
    "    ..."
   ],
   "id": "b7bbaf99afb66462",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's try passing environment variable DATASET_URL to a python script and look at different ways to do it.",
   "id": "23c932f62cd4d747"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T09:09:16.387025Z",
     "start_time": "2025-08-10T09:09:16.384109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%writefile connect_dataset.py\n",
    "import os\n",
    "\n",
    "\n",
    "def main():\n",
    "    dataset_url = os.environ.get('DATASET_URL', None)\n",
    "    print('DATASET_URL: ', dataset_url)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "id": "2c8c846e84b5dc14",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting connect_dataset.py\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T09:10:11.225660Z",
     "start_time": "2025-08-10T09:10:11.097742Z"
    }
   },
   "cell_type": "code",
   "source": "!python connect_dataset.py",
   "id": "fd4b9ee50816f98f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET_URL:  None\r\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T09:16:10.523851Z",
     "start_time": "2025-08-10T09:16:10.395789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from terminal:\n",
    "# DATASET_URL=<someurl> python connect_dataset.py\n",
    "\n",
    "# from jupyter notebook:\n",
    "import os\n",
    "os.environ['DATASET_URL'] = \"<someurl>\"\n",
    "!python connect_dataset.py"
   ],
   "id": "a716691deb60ca9a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET_URL:  <someurl>\r\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The standard way to store environment variables is in \".env\" files (which are always put in .gitignore for security).\n",
    "Dotnet package allows working with such files easily "
   ],
   "id": "1a61688489fb959a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T11:05:54.218462Z",
     "start_time": "2025-08-10T11:05:52.037133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "!pip install python-dotenv"
   ],
   "id": "2aa30510d8bcc9aa",
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T09:12:44.112174Z",
     "start_time": "2025-08-10T09:12:44.109208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%writefile .env\n",
    "DATASET_URL=somenewdataseturl"
   ],
   "id": "941af38bc40b9748",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing .env\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T09:16:13.601219Z",
     "start_time": "2025-08-10T09:16:13.598237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%writefile env_check.py\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "print('DATASET_URL:', os.environ.get('DATASET_URL'))\n",
    "load_dotenv('.env')\n",
    "print('DATASET_URL:', os.environ.get('DATASET_URL'))"
   ],
   "id": "e4adb36f3eca70c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting env_check.py\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T09:16:15.068066Z",
     "start_time": "2025-08-10T09:16:14.922553Z"
    }
   },
   "cell_type": "code",
   "source": "!python env_check.py",
   "id": "dcc40ba2dd6d80c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET_URL: <someurl>\r\n",
      "DATASET_URL: <someurl>\r\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Oops: when env variable is already declared, it is not overwritten by dotenv. Let's delete the old one first.",
   "id": "f74b88910ba46273"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T09:16:53.963398Z",
     "start_time": "2025-08-10T09:16:53.961014Z"
    }
   },
   "cell_type": "code",
   "source": "del os.environ['DATASET_URL']",
   "id": "e631c740503c2308",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T09:16:58.643632Z",
     "start_time": "2025-08-10T09:16:58.498080Z"
    }
   },
   "cell_type": "code",
   "source": "!python env_check.py",
   "id": "439b9b088c140b30",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET_URL: None\r\n",
      "DATASET_URL: somenewdataseturl\r\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Other ways to load configuration",
   "id": "41ee74de985d2d99"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Yaml",
   "id": "b02d008bc2073d0b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T09:20:23.170759Z",
     "start_time": "2025-08-10T09:20:23.167717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%writefile config.yaml\n",
    "database:\n",
    "  url: \"postgresql://user:pass@localhost:5432/db\"\n",
    "  timeout: 30\n",
    "logging:\n",
    "  level: \"DEBUG\""
   ],
   "id": "d451959b655eb262",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing config.yaml\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T11:06:01.100950Z",
     "start_time": "2025-08-10T11:05:58.971604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "!pip install pyyaml"
   ],
   "id": "7a205989d8494345",
   "outputs": [],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T09:20:34.097841Z",
     "start_time": "2025-08-10T09:20:34.094596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"config.yaml\", 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(config)\n",
    "print(config['database']['url'])"
   ],
   "id": "e7a35732429fa582",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'database': {'url': 'postgresql://user:pass@localhost:5432/db', 'timeout': 30}, 'logging': {'level': 'DEBUG'}}\n",
      "postgresql://user:pass@localhost:5432/db\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Json",
   "id": "8fbf048b000fe89d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T09:21:37.415399Z",
     "start_time": "2025-08-10T09:21:37.412530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%writefile config.json\n",
    "{\n",
    "    \"database\": {\n",
    "    \"url\": \"postgresql://user:pass@localhost:5432/db\",\n",
    "    \"timeout\": 30\n",
    "    },\n",
    "    \"logging\": {\n",
    "        \"level\": \"DEBUG\"\n",
    "    }\n",
    "}"
   ],
   "id": "b2ca86bbf9063e19",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing config.json\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T09:22:05.849150Z",
     "start_time": "2025-08-10T09:22:05.846332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(config)\n",
    "print(config['database']['url'])"
   ],
   "id": "d5bd5de188d19546",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'database': {'url': 'postgresql://user:pass@localhost:5432/db', 'timeout': 30}, 'logging': {'level': 'DEBUG'}}\n",
      "postgresql://user:pass@localhost:5432/db\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### INI",
   "id": "d0d298afde12c773"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T09:23:26.542761Z",
     "start_time": "2025-08-10T09:23:26.539349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%writefile config.ini\n",
    "[database]\n",
    "url = postgresql://user:pass@localhost:5432/db\n",
    "timeout = 30\n",
    "\n",
    "[logging]\n",
    "level = DEBUG"
   ],
   "id": "790c42ffad95dfa3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing config.ini\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T09:23:49.091077Z",
     "start_time": "2025-08-10T09:23:49.087791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from configparser import ConfigParser\n",
    "\n",
    "config = ConfigParser()\n",
    "config.read(\"config.ini\")\n",
    "\n",
    "print(config)\n",
    "print(config.get(\"database\", \"url\"))\n",
    "print(config.get(\"logging\", \"level\"))"
   ],
   "id": "61b2a15618275aaf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<configparser.ConfigParser object at 0x77925fac5510>\n",
      "postgresql://user:pass@localhost:5432/db\n",
      "DEBUG\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "95dff2e92f85d558"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Exercise: Analyzing Words in a Large Text File",
   "id": "223dbbd8c9e740c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We want to write an analyzer that processes given (possibly large) english text file at word level and extracts the following statistics:\n",
    "\n",
    "1) Total Number of words and unique words (form-independent)\n",
    "2) Number of words and unique words for each part of speech\n",
    "3) Number of characters in the longest noun\n",
    "4) Average \"positivity\" and \"negativity\" for the words\n",
    "\n",
    "Requirement: optional progress bar so that estimated time for processing is visible"
   ],
   "id": "d5dbb3afbb954242"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "An example of the input text files is shakespeare texts, downloaded from github (5 MB)",
   "id": "301e2d73b7b35e9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-08-09 11:27:06--  https://gist.githubusercontent.com/blakesanie/dde3a2b7e698f52f389532b4b52bc254/raw/76fe1b5e9efcf0d2afdfd78b0bfaa737ad0a67d3/shakespeare.txt\r\n",
      "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\r\n",
      "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 5436475 (5.2M) [text/plain]\r\n",
      "Saving to: ‘shakespeare.txt’\r\n",
      "\r\n",
      "shakespeare.txt     100%[===================>]   5.18M  1.40MB/s    in 3.7s    \r\n",
      "\r\n",
      "2025-08-09 11:27:11 (1.40 MB/s) - ‘shakespeare.txt’ saved [5436475/5436475]\r\n",
      "\r\n"
     ]
    }
   ],
   "execution_count": 37,
   "source": "!wget -O shakespeare.txt \"https://gist.githubusercontent.com/blakesanie/dde3a2b7e698f52f389532b4b52bc254/raw/76fe1b5e9efcf0d2afdfd78b0bfaa737ad0a67d3/shakespeare.txt\"",
   "id": "e04b78ac7db61888"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T09:28:37.082030Z",
     "start_time": "2025-08-10T09:28:36.968931Z"
    }
   },
   "cell_type": "code",
   "source": "!head shakespeare.txt",
   "id": "2e9e2510a4fb01fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  From fairest creatures we desire increase,\r\n",
      "  That thereby beauty's rose might never die,\r\n",
      "  But as the riper should by time decease,\r\n",
      "  His tender heir might bear his memory:\r\n",
      "  But thou contracted to thine own bright eyes,\r\n",
      "  Feed'st thy light's flame with self-substantial fuel,\r\n",
      "  Making a famine where abundance lies,\r\n",
      "  Thy self thy foe, to thy sweet self too cruel:\r\n",
      "  Thou that art now the world's fresh ornament,\r\n",
      "  And only herald to the gaudy spring,\r\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T09:28:46.846694Z",
     "start_time": "2025-08-10T09:28:46.823435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Bad example: don't do this with very large files\n",
    "with open(\"shakespeare.txt\") as f:\n",
    "    content = f.read()\n",
    "    ..."
   ],
   "id": "f1a7ad1ebc846852",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T09:28:47.257894Z",
     "start_time": "2025-08-10T09:28:47.254552Z"
    }
   },
   "cell_type": "code",
   "source": "len(content) # Very large",
   "id": "7a84d22ef83286c1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5436475"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T09:29:01.762550Z",
     "start_time": "2025-08-10T09:29:01.760143Z"
    }
   },
   "cell_type": "code",
   "source": "del content # let's delete it to free our memory",
   "id": "9ef3c58fec4a4e69",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bdad750c4cd22a2d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For the analysis of words, we are going to use spaCy and nltk --- very popular natural language processing libraries.",
   "id": "ea5cffa7e0102fac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T11:04:34.484410Z",
     "start_time": "2025-08-10T11:04:32.373521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "!pip install spacy nltk"
   ],
   "id": "d685e30e83092b9b",
   "outputs": [],
   "execution_count": 100
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T11:05:34.005873Z",
     "start_time": "2025-08-10T11:04:41.867082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "!python -m spacy download en_core_web_sm"
   ],
   "id": "7c7e0e90c5438adc",
   "outputs": [],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T11:05:38.186670Z",
     "start_time": "2025-08-10T11:05:35.812647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "!python -m nltk.downloader vader_lexicon"
   ],
   "id": "89bace79aa71e8e",
   "outputs": [],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T09:30:43.082053Z",
     "start_time": "2025-08-10T09:30:39.909157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example of using spacy\n",
    "import spacy\n",
    "\n",
    "text = \"usually , he would be tearing around the living room , playing with his toys .\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<10} | POS: {token.pos_:<8} | Explanation: {spacy.explain(token.pos_)}\")"
   ],
   "id": "ea37988ec8aeda80",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usually    | POS: ADV      | Explanation: adverb\n",
      ",          | POS: PUNCT    | Explanation: punctuation\n",
      "he         | POS: PRON     | Explanation: pronoun\n",
      "would      | POS: AUX      | Explanation: auxiliary\n",
      "be         | POS: AUX      | Explanation: auxiliary\n",
      "tearing    | POS: VERB     | Explanation: verb\n",
      "around     | POS: ADP      | Explanation: adposition\n",
      "the        | POS: DET      | Explanation: determiner\n",
      "living     | POS: NOUN     | Explanation: noun\n",
      "room       | POS: NOUN     | Explanation: noun\n",
      ",          | POS: PUNCT    | Explanation: punctuation\n",
      "playing    | POS: VERB     | Explanation: verb\n",
      "with       | POS: ADP      | Explanation: adposition\n",
      "his        | POS: PRON     | Explanation: pronoun\n",
      "toys       | POS: NOUN     | Explanation: noun\n",
      ".          | POS: PUNCT    | Explanation: punctuation\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T09:32:34.452145Z",
     "start_time": "2025-08-10T09:32:34.445704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example of using nltk for sentiment analysis\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "print(sia.polarity_scores(text))\n",
    "\n",
    "for word in ['ok', 'he', 'good', 'bad', 'strong', 'weak', 'terrible', 'beautiful', 'awful']:\n",
    "    print(f\"Sentiment for word {word}: {sia.polarity_scores(word)}\")"
   ],
   "id": "91ffb85343211711",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.87, 'pos': 0.13, 'compound': 0.2023}\n",
      "Sentiment for word ok: {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.296}\n",
      "Sentiment for word he: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "Sentiment for word good: {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4404}\n",
      "Sentiment for word bad: {'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.5423}\n",
      "Sentiment for word strong: {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.5106}\n",
      "Sentiment for word weak: {'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.4404}\n",
      "Sentiment for word terrible: {'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.4767}\n",
      "Sentiment for word beautiful: {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.5994}\n",
      "Sentiment for word awful: {'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.4588}\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "First, let's write word data extraction code: enumerate_words(file_path: str)\n",
    "token_to_word function is already given that processes spaCy's Token and extracts all required data."
   ],
   "id": "4cc1560c501032a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T10:14:36.673687Z",
     "start_time": "2025-08-10T10:14:36.669185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Generator\n",
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Word:\n",
    "    content: str\n",
    "    pos_tag: str\n",
    "    lemma: str | None\n",
    "    positivity: float\n",
    "    negativity: float\n",
    "\n",
    "\n",
    "def token_to_word(token: Token) -> Word | None:\n",
    "    if token.is_punct:\n",
    "        return None\n",
    "    polarity_scores = sia.polarity_scores(token.text)\n",
    "    return Word(\n",
    "        content=token.text,\n",
    "        pos_tag=token.pos_,\n",
    "        lemma=token.lemma_ if not token.is_punct else None,\n",
    "        positivity=polarity_scores['pos'],\n",
    "        negativity=polarity_scores['neg']\n",
    "    )"
   ],
   "id": "bfea5ceb81f0260b",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 52,
   "source": [
    "def enumerate_words_version1(file_path: str) -> list[Word]:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    result = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            for token in nlp(line):\n",
    "                word = token_to_word(token)\n",
    "                if word is not None:\n",
    "                    result.append(word)\n",
    "    return result"
   ],
   "id": "448ae3e96218a4a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## What's wrong with the first version?",
   "id": "6ca9c581c51a104c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1) Memory inefficient: we don't need to store the list of all words, if we are only doing a for loop.\n",
    "2) It's better to process text and replace all punctuation marks and non-english letters with spaces.\n",
    "3) No progress bar: how long will it take? Who knows..."
   ],
   "id": "443a199ecf28a04c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T10:32:28.408533Z",
     "start_time": "2025-08-10T10:32:28.403259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Iterator\n",
    "\n",
    "class WordIteratorVersion2:\n",
    "    def __init__(self, file_path: str):\n",
    "        self.file_path = file_path\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.file_handle = None\n",
    "        self.token_iterator = None\n",
    "    \n",
    "    def __iter__(self) -> Iterator[Word]:\n",
    "        self.file_handle = open(self.file_path, 'r')\n",
    "        return self\n",
    "    \n",
    "    def __next__(self) -> Word:\n",
    "        while True:\n",
    "            if self.token_iterator is None:\n",
    "                current_line = self.file_handle.readline()\n",
    "                \n",
    "                if not current_line:\n",
    "                    self.file_handle.close()\n",
    "                    raise StopIteration\n",
    "                \n",
    "                processed_line = re.sub(r'[^a-zA-Z]', ' ', current_line)\n",
    "                self.token_iterator = iter(self.nlp(processed_line))\n",
    "            try:\n",
    "                token = next(self.token_iterator)\n",
    "                word = token_to_word(token)\n",
    "                if word is not None:\n",
    "                    return word\n",
    "            except StopIteration:\n",
    "                self.token_iterator = None\n",
    "    \n",
    "    def __del__(self):\n",
    "        if self.file_handle and not self.file_handle.closed:\n",
    "            self.file_handle.close()"
   ],
   "id": "30b087822142ae24",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T10:33:38.560605Z",
     "start_time": "2025-08-10T10:33:38.166425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "i = 0\n",
    "for word in WordIteratorVersion2(\"shakespeare.txt\"):\n",
    "    print(word)\n",
    "    i += 1\n",
    "    if i >= 50:\n",
    "        break"
   ],
   "id": "320ff7b839551d0d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word(content='  ', pos_tag='SPACE', lemma='  ', positivity=0.0, negativity=0.0)\n",
      "Word(content='From', pos_tag='ADP', lemma='from', positivity=0.0, negativity=0.0)\n",
      "Word(content='fairest', pos_tag='ADJ', lemma='fair', positivity=0.0, negativity=0.0)\n",
      "Word(content='creatures', pos_tag='NOUN', lemma='creature', positivity=0.0, negativity=0.0)\n",
      "Word(content='we', pos_tag='PRON', lemma='we', positivity=0.0, negativity=0.0)\n",
      "Word(content='desire', pos_tag='AUX', lemma='desire', positivity=1.0, negativity=0.0)\n",
      "Word(content='increase', pos_tag='NOUN', lemma='increase', positivity=1.0, negativity=0.0)\n",
      "Word(content=' ', pos_tag='SPACE', lemma=' ', positivity=0.0, negativity=0.0)\n",
      "Word(content='  ', pos_tag='SPACE', lemma='  ', positivity=0.0, negativity=0.0)\n",
      "Word(content='That', pos_tag='SCONJ', lemma='that', positivity=0.0, negativity=0.0)\n",
      "Word(content='thereby', pos_tag='ADV', lemma='thereby', positivity=0.0, negativity=0.0)\n",
      "Word(content='beauty', pos_tag='NOUN', lemma='beauty', positivity=1.0, negativity=0.0)\n",
      "Word(content='s', pos_tag='PROPN', lemma='s', positivity=0.0, negativity=0.0)\n",
      "Word(content='rose', pos_tag='VERB', lemma='rise', positivity=0.0, negativity=0.0)\n",
      "Word(content='might', pos_tag='AUX', lemma='might', positivity=0.0, negativity=0.0)\n",
      "Word(content='never', pos_tag='ADV', lemma='never', positivity=0.0, negativity=0.0)\n",
      "Word(content='die', pos_tag='VERB', lemma='die', positivity=0.0, negativity=1.0)\n",
      "Word(content=' ', pos_tag='SPACE', lemma=' ', positivity=0.0, negativity=0.0)\n",
      "Word(content='  ', pos_tag='SPACE', lemma='  ', positivity=0.0, negativity=0.0)\n",
      "Word(content='But', pos_tag='CCONJ', lemma='but', positivity=0.0, negativity=0.0)\n",
      "Word(content='as', pos_tag='SCONJ', lemma='as', positivity=0.0, negativity=0.0)\n",
      "Word(content='the', pos_tag='DET', lemma='the', positivity=0.0, negativity=0.0)\n",
      "Word(content='riper', pos_tag='NOUN', lemma='riper', positivity=0.0, negativity=0.0)\n",
      "Word(content='should', pos_tag='AUX', lemma='should', positivity=0.0, negativity=0.0)\n",
      "Word(content='by', pos_tag='ADP', lemma='by', positivity=0.0, negativity=0.0)\n",
      "Word(content='time', pos_tag='NOUN', lemma='time', positivity=0.0, negativity=0.0)\n",
      "Word(content='decease', pos_tag='NOUN', lemma='decease', positivity=0.0, negativity=0.0)\n",
      "Word(content=' ', pos_tag='SPACE', lemma=' ', positivity=0.0, negativity=0.0)\n",
      "Word(content='  ', pos_tag='SPACE', lemma='  ', positivity=0.0, negativity=0.0)\n",
      "Word(content='His', pos_tag='PRON', lemma='his', positivity=0.0, negativity=0.0)\n",
      "Word(content='tender', pos_tag='NOUN', lemma='tender', positivity=0.0, negativity=0.0)\n",
      "Word(content='heir', pos_tag='NOUN', lemma='heir', positivity=0.0, negativity=0.0)\n",
      "Word(content='might', pos_tag='AUX', lemma='might', positivity=0.0, negativity=0.0)\n",
      "Word(content='bear', pos_tag='VERB', lemma='bear', positivity=0.0, negativity=0.0)\n",
      "Word(content='his', pos_tag='PRON', lemma='his', positivity=0.0, negativity=0.0)\n",
      "Word(content='memory', pos_tag='NOUN', lemma='memory', positivity=0.0, negativity=0.0)\n",
      "Word(content=' ', pos_tag='SPACE', lemma=' ', positivity=0.0, negativity=0.0)\n",
      "Word(content='  ', pos_tag='SPACE', lemma='  ', positivity=0.0, negativity=0.0)\n",
      "Word(content='But', pos_tag='CCONJ', lemma='but', positivity=0.0, negativity=0.0)\n",
      "Word(content='thou', pos_tag='PROPN', lemma='thou', positivity=0.0, negativity=0.0)\n",
      "Word(content='contracted', pos_tag='VERB', lemma='contract', positivity=0.0, negativity=0.0)\n",
      "Word(content='to', pos_tag='PART', lemma='to', positivity=0.0, negativity=0.0)\n",
      "Word(content='thine', pos_tag='VERB', lemma='thine', positivity=0.0, negativity=0.0)\n",
      "Word(content='own', pos_tag='ADJ', lemma='own', positivity=0.0, negativity=0.0)\n",
      "Word(content='bright', pos_tag='ADJ', lemma='bright', positivity=1.0, negativity=0.0)\n",
      "Word(content='eyes', pos_tag='NOUN', lemma='eye', positivity=0.0, negativity=0.0)\n",
      "Word(content=' ', pos_tag='SPACE', lemma=' ', positivity=0.0, negativity=0.0)\n",
      "Word(content='  ', pos_tag='SPACE', lemma='  ', positivity=0.0, negativity=0.0)\n",
      "Word(content='Feed', pos_tag='PROPN', lemma='Feed', positivity=0.0, negativity=0.0)\n",
      "Word(content='st', pos_tag='PROPN', lemma='st', positivity=0.0, negativity=0.0)\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This method is VERY code-intensive. We can do it much easier with Generator",
   "id": "a15d803bfa0b66cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 75,
   "source": [
    "def enumerate_words_version2(file_path: str) -> Generator[Word, None, None]:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            processed_line = re.sub(r'[^a-zA-Z]', ' ', line)\n",
    "            for token in nlp(processed_line):\n",
    "                word = token_to_word(token)\n",
    "                if word is not None:\n",
    "                    yield word"
   ],
   "id": "7875d27bb677f696"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T10:33:50.502577Z",
     "start_time": "2025-08-10T10:33:50.127068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "i = 0\n",
    "for word in enumerate_words_version2(\"shakespeare.txt\"):\n",
    "    print(word)\n",
    "    i += 1\n",
    "    if i >= 50:\n",
    "        break"
   ],
   "id": "7746161ec3088544",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word(content='  ', pos_tag='SPACE', lemma='  ', positivity=0.0, negativity=0.0)\n",
      "Word(content='From', pos_tag='ADP', lemma='from', positivity=0.0, negativity=0.0)\n",
      "Word(content='fairest', pos_tag='ADJ', lemma='fair', positivity=0.0, negativity=0.0)\n",
      "Word(content='creatures', pos_tag='NOUN', lemma='creature', positivity=0.0, negativity=0.0)\n",
      "Word(content='we', pos_tag='PRON', lemma='we', positivity=0.0, negativity=0.0)\n",
      "Word(content='desire', pos_tag='AUX', lemma='desire', positivity=1.0, negativity=0.0)\n",
      "Word(content='increase', pos_tag='NOUN', lemma='increase', positivity=1.0, negativity=0.0)\n",
      "Word(content=' ', pos_tag='SPACE', lemma=' ', positivity=0.0, negativity=0.0)\n",
      "Word(content='  ', pos_tag='SPACE', lemma='  ', positivity=0.0, negativity=0.0)\n",
      "Word(content='That', pos_tag='SCONJ', lemma='that', positivity=0.0, negativity=0.0)\n",
      "Word(content='thereby', pos_tag='ADV', lemma='thereby', positivity=0.0, negativity=0.0)\n",
      "Word(content='beauty', pos_tag='NOUN', lemma='beauty', positivity=1.0, negativity=0.0)\n",
      "Word(content='s', pos_tag='PROPN', lemma='s', positivity=0.0, negativity=0.0)\n",
      "Word(content='rose', pos_tag='VERB', lemma='rise', positivity=0.0, negativity=0.0)\n",
      "Word(content='might', pos_tag='AUX', lemma='might', positivity=0.0, negativity=0.0)\n",
      "Word(content='never', pos_tag='ADV', lemma='never', positivity=0.0, negativity=0.0)\n",
      "Word(content='die', pos_tag='VERB', lemma='die', positivity=0.0, negativity=1.0)\n",
      "Word(content=' ', pos_tag='SPACE', lemma=' ', positivity=0.0, negativity=0.0)\n",
      "Word(content='  ', pos_tag='SPACE', lemma='  ', positivity=0.0, negativity=0.0)\n",
      "Word(content='But', pos_tag='CCONJ', lemma='but', positivity=0.0, negativity=0.0)\n",
      "Word(content='as', pos_tag='SCONJ', lemma='as', positivity=0.0, negativity=0.0)\n",
      "Word(content='the', pos_tag='DET', lemma='the', positivity=0.0, negativity=0.0)\n",
      "Word(content='riper', pos_tag='NOUN', lemma='riper', positivity=0.0, negativity=0.0)\n",
      "Word(content='should', pos_tag='AUX', lemma='should', positivity=0.0, negativity=0.0)\n",
      "Word(content='by', pos_tag='ADP', lemma='by', positivity=0.0, negativity=0.0)\n",
      "Word(content='time', pos_tag='NOUN', lemma='time', positivity=0.0, negativity=0.0)\n",
      "Word(content='decease', pos_tag='NOUN', lemma='decease', positivity=0.0, negativity=0.0)\n",
      "Word(content=' ', pos_tag='SPACE', lemma=' ', positivity=0.0, negativity=0.0)\n",
      "Word(content='  ', pos_tag='SPACE', lemma='  ', positivity=0.0, negativity=0.0)\n",
      "Word(content='His', pos_tag='PRON', lemma='his', positivity=0.0, negativity=0.0)\n",
      "Word(content='tender', pos_tag='NOUN', lemma='tender', positivity=0.0, negativity=0.0)\n",
      "Word(content='heir', pos_tag='NOUN', lemma='heir', positivity=0.0, negativity=0.0)\n",
      "Word(content='might', pos_tag='AUX', lemma='might', positivity=0.0, negativity=0.0)\n",
      "Word(content='bear', pos_tag='VERB', lemma='bear', positivity=0.0, negativity=0.0)\n",
      "Word(content='his', pos_tag='PRON', lemma='his', positivity=0.0, negativity=0.0)\n",
      "Word(content='memory', pos_tag='NOUN', lemma='memory', positivity=0.0, negativity=0.0)\n",
      "Word(content=' ', pos_tag='SPACE', lemma=' ', positivity=0.0, negativity=0.0)\n",
      "Word(content='  ', pos_tag='SPACE', lemma='  ', positivity=0.0, negativity=0.0)\n",
      "Word(content='But', pos_tag='CCONJ', lemma='but', positivity=0.0, negativity=0.0)\n",
      "Word(content='thou', pos_tag='PROPN', lemma='thou', positivity=0.0, negativity=0.0)\n",
      "Word(content='contracted', pos_tag='VERB', lemma='contract', positivity=0.0, negativity=0.0)\n",
      "Word(content='to', pos_tag='PART', lemma='to', positivity=0.0, negativity=0.0)\n",
      "Word(content='thine', pos_tag='VERB', lemma='thine', positivity=0.0, negativity=0.0)\n",
      "Word(content='own', pos_tag='ADJ', lemma='own', positivity=0.0, negativity=0.0)\n",
      "Word(content='bright', pos_tag='ADJ', lemma='bright', positivity=1.0, negativity=0.0)\n",
      "Word(content='eyes', pos_tag='NOUN', lemma='eye', positivity=0.0, negativity=0.0)\n",
      "Word(content=' ', pos_tag='SPACE', lemma=' ', positivity=0.0, negativity=0.0)\n",
      "Word(content='  ', pos_tag='SPACE', lemma='  ', positivity=0.0, negativity=0.0)\n",
      "Word(content='Feed', pos_tag='PROPN', lemma='Feed', positivity=0.0, negativity=0.0)\n",
      "Word(content='st', pos_tag='PROPN', lemma='st', positivity=0.0, negativity=0.0)\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for word in enumerate_words_version2(\"shakespeare.txt\"):\n",
    "    ... # How long?"
   ],
   "id": "1f563aa1ad8f83df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Adding tqdm",
   "id": "3b99aa26a522192d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T10:14:40.710063Z",
     "start_time": "2025-08-10T10:14:40.705501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm.notebook import tqdm\n",
    "def enumerate_words_version3(file_path: str, pbar: bool = False) -> Generator[Word, None, None]:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    if pbar:\n",
    "        with open(file_path, 'r') as f:\n",
    "            line_count = sum(1 for line in f)\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        line_iterable = tqdm(f, total=line_count) if pbar else f\n",
    "        for line in line_iterable:\n",
    "            processed_line = re.sub(r'[^a-zA-Z]', ' ', line)\n",
    "            for token in nlp(processed_line):\n",
    "                word = token_to_word(token)\n",
    "                if word is not None:\n",
    "                    yield word"
   ],
   "id": "cf7dacabf6a17554",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T11:07:23.103236Z",
     "start_time": "2025-08-10T11:07:19.110719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    for word in enumerate_words_version3(\"shakespeare.txt\", pbar=True):\n",
    "        ...\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted\")"
   ],
   "id": "3711e3354ef0883d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/124185 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "11974dbd8c724ed7b8bdd48207d5289d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interrupted\n"
     ]
    }
   ],
   "execution_count": 106
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Improving speed",
   "id": "f780c68d3b09b86"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now it takes about 7 minutes for our 5 MB file. Can we make it faster?\n",
    "\n",
    "The answer is yes!\n",
    "\n",
    "First, let's recognise the bottleneck of our function. It is nlp(processed_line), which does some complex stuff. Calling it with a larger string is faster than calling multiple times on small strings. So, let's implement batches."
   ],
   "id": "b8a2a61b2bddd366"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T10:35:31.626002Z",
     "start_time": "2025-08-10T10:35:31.621235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from io import StringIO\n",
    "import math\n",
    "\n",
    "\n",
    "def enumerate_batches(file_path: str, batch_size: int) -> Generator[str, None, None]:\n",
    "    with open(file_path, 'r') as f:\n",
    "        builder = StringIO()\n",
    "        counter = 0\n",
    "        for line in f:\n",
    "            builder.write(line)\n",
    "            builder.write(\" \")\n",
    "            counter += len(line)\n",
    "            if counter >= batch_size:\n",
    "                yield builder.getvalue()\n",
    "                builder = StringIO()\n",
    "                counter = 0\n",
    "    # important: do not forget the ending when batching\n",
    "    if counter > 0:\n",
    "        yield builder.getvalue()\n",
    "\n",
    "\n",
    "def enumerate_words_version4(file_path: str, batch_size: int, pbar: bool = False) -> Generator[Word, None, None]:\n",
    "    batch_iterable = enumerate_batches(file_path, batch_size)\n",
    "    if pbar:\n",
    "        total_size = os.path.getsize(file_path)\n",
    "        batch_count = int(math.ceil(total_size / batch_size))\n",
    "        batch_iterable = tqdm(enumerate_batches(file_path, batch_size), total=batch_count)\n",
    "    for batch in batch_iterable:\n",
    "        processed_batch = re.sub(r'[^a-zA-Z]', ' ', batch)\n",
    "        for token in nlp(processed_batch):\n",
    "            word = token_to_word(token)\n",
    "            if word is not None:\n",
    "                yield word"
   ],
   "id": "c65d6221038faf3e",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T11:07:56.456297Z",
     "start_time": "2025-08-10T11:07:52.412391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    for word in enumerate_words_version4(\"shakespeare.txt\", batch_size=10000, pbar=True):\n",
    "        ...\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted')"
   ],
   "id": "2966be4e72ab32fd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/544 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0a9a4254d1534fcb9213bafc5678a00d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interrupted\n"
     ]
    }
   ],
   "execution_count": 108
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Nice! Now it's 2 minutes. But, luckily, there is a function nlp.pipe, which does practically the same as we do, but also includes multiprocessing out of the box. So, we can pass our processed lines generator in it.\n",
    "\n",
    "Another optimization is: we do not need Named Entity Recognition (ner), so we can disable that component, thus improving the speed slightly. "
   ],
   "id": "c226a551cc295219"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T10:37:32.753744Z",
     "start_time": "2025-08-10T10:37:32.749904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def enumerate_processed_lines(file_path: str, pbar: bool = False) -> Generator[str, None, None]:\n",
    "    if pbar:\n",
    "        with open(file_path, 'r') as f:\n",
    "            line_count = sum(1 for line in f)\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        line_iterable = tqdm(f, total=line_count) if pbar else f\n",
    "        for line in line_iterable:\n",
    "            processed_line = re.sub(r'[^a-zA-Z]', ' ', line)\n",
    "            yield processed_line\n",
    "\n",
    "\n",
    "def enumerate_words(\n",
    "        file_path: str, \n",
    "        batch_size: int = 1000, \n",
    "        n_process: int = 4, \n",
    "        pbar: bool = False\n",
    ") -> Generator[Word, None, None]:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    for doc in nlp.pipe(enumerate_processed_lines(file_path, pbar), batch_size=batch_size, n_process=n_process, disable=['ner']):\n",
    "        for token in doc:\n",
    "            word = token_to_word(token)\n",
    "            if word is not None:\n",
    "                yield word"
   ],
   "id": "e9a95bf9872eca64",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T11:10:02.145222Z",
     "start_time": "2025-08-10T11:10:02.143567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    for word in enumerate_words(\"shakespeare.txt\", pbar=True):\n",
    "        ...\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted')"
   ],
   "id": "57854a0a6e0c15b4",
   "outputs": [],
   "execution_count": 111
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Finally, it's about 1 minute. And we're done with enumerate_words function.\n",
    "\n",
    "The next step is calculating word statistics, which is pretty straightforward."
   ],
   "id": "42003cf8895d3618"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T09:20:05.127959Z",
     "start_time": "2025-08-09T09:18:24.088695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_word_count = 0\n",
    "unique_words = set()\n",
    "pos_data = {}\n",
    "longest_noun_word = \"\"\n",
    "total_positivity = 0\n",
    "total_negativity = 0\n",
    "\n",
    "for word in enumerate_words(\"shakespeare.txt\", pbar=True):\n",
    "    total_word_count += 1\n",
    "    if word.pos_tag not in pos_data:\n",
    "        pos_data[word.pos_tag] = { 'total': 0, 'unique': set() }\n",
    "    word_pos_data = pos_data[word.pos_tag]\n",
    "    word_pos_data['total'] += 1\n",
    "    if word.lemma is not None:\n",
    "        unique_words.add(word.lemma)\n",
    "        word_pos_data['unique'].add(word.lemma)\n",
    "    if word.pos_tag == 'NOUN' and len(word.content) > len(longest_noun_word):\n",
    "        longest_noun_word = word.content\n",
    "    total_positivity += word.positivity\n",
    "    total_negativity += word.negativity"
   ],
   "id": "5b79e91b09ab0bc9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/124185 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "153d2b864df84ff4ab29ec245c01278d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T09:20:05.133817Z",
     "start_time": "2025-08-09T09:20:05.129544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Total words: {total_word_count}\")\n",
    "print(f\"Unique words: {len(unique_words)}\")\n",
    "print(\"-----------------\")\n",
    "for pos_tag, data in pos_data.items():\n",
    "    print(f\"{spacy.explain(pos_tag)}: Total {data['total']}, Unique {len(data['unique'])}\")\n",
    "print(\"-----------------\")\n",
    "print(f\"Total positivity: {total_positivity}, fraction: {total_positivity / total_word_count:.3f}\")\n",
    "print(f\"Total negativity: {total_negativity}, fraction: {total_negativity / total_word_count:.3f}\")\n",
    "print(f\"Longest noun ({len(longest_noun_word)} chars): {longest_noun_word}\")"
   ],
   "id": "5aa91b2d8ef8176f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 1255638\n",
      "Unique words: 22181\n",
      "-----------------\n",
      "space: Total 328512, Unique 59\n",
      "adposition: Total 81663, Unique 170\n",
      "adjective: Total 54843, Unique 3969\n",
      "noun: Total 171204, Unique 10053\n",
      "pronoun: Total 144726, Unique 113\n",
      "auxiliary: Total 56471, Unique 195\n",
      "subordinating conjunction: Total 23992, Unique 65\n",
      "adverb: Total 42796, Unique 1165\n",
      "proper noun: Total 93483, Unique 7862\n",
      "verb: Total 120710, Unique 5088\n",
      "coordinating conjunction: Total 37638, Unique 25\n",
      "determiner: Total 59433, Unique 43\n",
      "particle: Total 24256, Unique 7\n",
      "numeral: Total 4386, Unique 41\n",
      "other: Total 5574, Unique 121\n",
      "interjection: Total 5941, Unique 119\n",
      "symbol: Total 2, Unique 2\n",
      "punctuation: Total 8, Unique 2\n",
      "-----------------\n",
      "Total positivity: 47714.0, fraction: 0.038\n",
      "Total negativity: 35451.0, fraction: 0.028\n",
      "Longest noun (16 chars): incomprehensible\n"
     ]
    }
   ],
   "execution_count": 99
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Wrapping Word Analyzer into an Package",
   "id": "ba5fbedc6d330fc9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, let's create a package with CLI interface for our word analyzing function.",
   "id": "143f9e52c683fccd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Main file",
   "id": "8829d553e04acdb5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First, let's modify our code a little bit, to return data class with all the statistics",
   "id": "bd3db361bcebef9b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T10:41:23.032528Z",
     "start_time": "2025-08-10T10:41:23.028398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%writefile worddata.py\n",
    "import re\n",
    "from dataclasses import dataclass, field\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from typing import Set, Generator\n",
    "from spacy.tokens import Token\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Word:\n",
    "    content: str\n",
    "    pos_tag: str\n",
    "    lemma: str | None\n",
    "    positivity: float\n",
    "    negativity: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class POSFrequencyData:\n",
    "    total: int = 0\n",
    "    unique: Set[str] = field(default_factory=lambda: set())\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class WordData:\n",
    "    total_word_count: int = 0\n",
    "    unique_words: Set[str] = field(default_factory=lambda: set())\n",
    "    pos_frequency_data: dict[str, POSFrequencyData] = field(default_factory=lambda: {})\n",
    "    longest_noun_word: str = \"\"\n",
    "    total_positivity: float = 0\n",
    "    total_negativity: float = 0\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        result = f\"Total words: {self.total_word_count}\\n\"\n",
    "        result += f\"Unique words: {len(self.unique_words)}\\n\"\n",
    "        result += \"-----------------\\n\"\n",
    "        for pos_tag, data in self.pos_frequency_data.items():\n",
    "            result += f\"{spacy.explain(pos_tag)}: Total {data.total}, Unique {len(data.unique)}\\n\"\n",
    "        result += \"-----------------\\n\"\n",
    "        result += f\"Total positivity: {self.total_positivity}, fraction: {self.total_positivity / self.total_word_count:.3f}\\n\"\n",
    "        result += f\"Total negativity: {self.total_negativity}, fraction: {self.total_negativity / self.total_word_count:.3f}\\n\"\n",
    "        result += f\"Longest noun ({len(self.longest_noun_word)} chars): {self.longest_noun_word}\\n\"\n",
    "        return result\n",
    "\n",
    "\n",
    "def token_to_word(token: Token, sia: SentimentIntensityAnalyzer) -> Word | None:\n",
    "    if token.is_punct:\n",
    "        return None\n",
    "    polarity_scores = sia.polarity_scores(token.text)\n",
    "    return Word(\n",
    "        content=token.text,\n",
    "        pos_tag=token.pos_,\n",
    "        lemma=token.lemma_ if not token.is_punct else None,\n",
    "        positivity=polarity_scores['pos'],\n",
    "        negativity=polarity_scores['neg']\n",
    "    )\n",
    "\n",
    "\n",
    "def enumerate_processed_lines(file_path: str, pbar: bool = False) -> Generator[str, None, None]:\n",
    "    if pbar:\n",
    "        with open(file_path, 'r') as f:\n",
    "            line_count = sum(1 for line in f)\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        line_iterable = tqdm(f, total=line_count) if pbar else f\n",
    "        for line in line_iterable:\n",
    "            processed_line = re.sub(r'[^a-zA-Z]', ' ', line)\n",
    "            yield processed_line\n",
    "\n",
    "\n",
    "def enumerate_words(\n",
    "        file_path: str,\n",
    "        batch_size: int = 1000,\n",
    "        n_process: int = 4,\n",
    "        pbar: bool = False\n",
    ") -> Generator[Word, None, None]:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    for doc in nlp.pipe(enumerate_processed_lines(file_path, pbar), batch_size=batch_size, n_process=n_process, disable=['ner']):\n",
    "        for token in doc:\n",
    "            word = token_to_word(token, sia)\n",
    "            if word is not None:\n",
    "                yield word\n",
    "\n",
    "\n",
    "def get_word_data(\n",
    "        file_path: str,\n",
    "        batch_size: int = 1000,\n",
    "        n_process: int = 4,\n",
    "        pbar: bool = False\n",
    ") -> WordData:\n",
    "    result = WordData()\n",
    "\n",
    "    for word in enumerate_words(file_path, batch_size=batch_size, n_process=n_process, pbar=pbar):\n",
    "        result.total_word_count += 1\n",
    "        if word.pos_tag not in result.pos_frequency_data:\n",
    "            result.pos_frequency_data[word.pos_tag] = POSFrequencyData()\n",
    "        word_pos_data = result.pos_frequency_data[word.pos_tag]\n",
    "        word_pos_data.total += 1\n",
    "        if word.lemma is not None:\n",
    "            result.unique_words.add(word.lemma)\n",
    "            word_pos_data.unique.add(word.lemma)\n",
    "        if word.pos_tag == 'NOUN' and len(word.content) > len(result.longest_noun_word):\n",
    "            result.longest_noun_word = word.content\n",
    "        result.total_positivity += word.positivity\n",
    "        result.total_negativity += word.negativity\n",
    "\n",
    "    return result\n"
   ],
   "id": "bdc36e835b35c5fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting worddata.py\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## CLI",
   "id": "fadc5016e57f849"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We're gonna write 2 versions of CLI interface: using argparser and using click, which are 2 most famous python CLI libraries.",
   "id": "15f3e0e9ced6dfc5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Argparser example",
   "id": "70ddec8353f7c79d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T10:43:30.064039Z",
     "start_time": "2025-08-10T10:43:30.060311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%writefile cli_argparser.py\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "from worddata import get_word_data\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Word analyzer\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"file_path\",\n",
    "        type=str,\n",
    "        help=\"Path to input file\"\n",
    "    )\n",
    "\n",
    "    # Optional arguments with defaults\n",
    "    parser.add_argument(\n",
    "        \"--batch-size\",\n",
    "        type=int,\n",
    "        default=1000,\n",
    "        help=\"Number of items per batch\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--n-process\",\n",
    "        type=int,\n",
    "        default=4,\n",
    "        help=\"Number of parallel processes\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--no-pbar\",\n",
    "        action=\"store_false\",\n",
    "        dest=\"pbar\",\n",
    "        help=\"Disable progress bar\"\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if not os.path.exists(args.file_path):\n",
    "        parser.error(f\"File not found: {args.file_path}\")\n",
    "\n",
    "    word_data = get_word_data(\n",
    "        file_path=args.file_path,\n",
    "        batch_size=args.batch_size,\n",
    "        n_process=args.n_process,\n",
    "        pbar=args.pbar\n",
    "    )\n",
    "\n",
    "    print(word_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "ed4678c72d359d7f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cli_argparser.py\n"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Click example",
   "id": "b7fe2ebfbffa2b12"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T10:43:54.614591Z",
     "start_time": "2025-08-10T10:43:54.611174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%writefile cli_click.py\n",
    "import click\n",
    "import os\n",
    "from worddata import get_word_data\n",
    "\n",
    "\n",
    "@click.command()\n",
    "@click.argument(\n",
    "    \"file_path\",\n",
    "    type=click.Path(exists=True, dir_okay=False, readable=True),\n",
    "    required=True\n",
    ")\n",
    "@click.option(\n",
    "    \"--batch-size\",\n",
    "    type=int,\n",
    "    default=1000,\n",
    "    show_default=True,\n",
    "    help=\"Number of items per batch\"\n",
    ")\n",
    "@click.option(\n",
    "    \"--n-process\",\n",
    "    type=int,\n",
    "    default=4,\n",
    "    show_default=True,\n",
    "    help=\"Number of parallel processes\"\n",
    ")\n",
    "@click.option(\n",
    "    \"--no-pbar\",\n",
    "    is_flag=True,\n",
    "    default=False,\n",
    "    help=\"Disable progress bar\"\n",
    ")\n",
    "def main(file_path: str, batch_size: int, n_process: int, no_pbar: bool):\n",
    "    click.echo(\"Start processing\")\n",
    "    word_data = get_word_data(\n",
    "        file_path=file_path,\n",
    "        batch_size=batch_size,\n",
    "        n_process=n_process,\n",
    "        pbar=not no_pbar\n",
    "    )\n",
    "\n",
    "    click.echo(\"Word Analysis Results:\")\n",
    "    click.echo(click.style(str(word_data), fg='green'))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "e16134d1d46d2eba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cli_click.py\n"
     ]
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Wrapping it all into a package",
   "id": "331c639784a1b5a1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*This will be shown externally in word_analyzer directory*",
   "id": "e73fd9b74072c525"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Installing our package into another environment",
   "id": "3d0eb027ac95ce64"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Will be shown in another notebook*",
   "id": "38121a22e2996b85"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "851468c776cb36fe"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
